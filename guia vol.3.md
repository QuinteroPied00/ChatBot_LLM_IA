# üìå Curso Pr√°ctico: Construcci√≥n de un Chatbot con IA Generativa y Base de Datos Vectorial

Este curso guiar√° a los estudiantes en la creaci√≥n de un **chatbot con IA generativa** utilizando **LLaMA, LangChain, ChromaDB y Streamlit**. Se abordar√° desde la instalaci√≥n y configuraci√≥n del entorno hasta la optimizaci√≥n de respuestas a trav√©s de t√©cnicas avanzadas de **ingenier√≠a de prompts**.

---

## **1Ô∏è‚É£ Introducci√≥n a la IA Generativa y Bases de Datos Vectoriales**

### **üìå Inteligencia Artificial Generativa**   

La IA generativa hace referencia a modelos capaces de **generar texto, im√°genes, c√≥digo y otros tipos de contenido** basado en patrones aprendidos a partir de grandes vol√∫menes de datos. Modelos como **LLaMA** utilizan arquitecturas de **Transformers**, que permiten procesar informaci√≥n secuencialmente, atendiendo relaciones contextuales dentro del texto.

### **üìå Bases de Datos Vectoriales**

Las bases de datos vectoriales, como **ChromaDB**, permiten almacenar informaci√≥n en forma de **vectores embebidos**. Esto facilita la b√∫squeda y recuperaci√≥n eficiente de informaci√≥n basada en **similitud sem√°ntica**, en lugar de coincidencias exactas de palabras clave.

üìç **C√≥mo lo implementamos en este curso:**

- mejoraUtilizamos **HuggingFaceEmbeddings** para convertir el texto en representaciones vectoriales.
- Almacenamos estos vectores en **ChromaDB** para facilitar consultas en lenguaje natural.
- Integramos estas consultas en **LangChain**, combinando bases de datos y modelos generativos para generar respuestas relevantes.

---

## **2Ô∏è‚É£ Instalaci√≥n del Entorno de Desarrollo**

### **üîπ Paso 1: Crear y Activar un Entorno Virtual**

```bash
# Crear un entorno virtual
python3 -m venv venv

# Activar el entorno virtual (Linux/macOS)
source venv/bin/activate

# Activar el entorno virtual (Windows)
venv\Scripts\activate
```

### **üîπ Paso 2: Instalar las Dependencias**

```bash
pip install streamlit langchain langchain-community ollama chromadb sentence-transformers
```

---

## **3Ô∏è‚É£ Configuraci√≥n del Logger**

Para facilitar la depuraci√≥n y monitoreo del chatbot, configuramos un sistema de **logging** que capture eventos clave en la ejecuci√≥n.

üìç **Archivo ****`logger.py`**:

```python
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
```

---

## **4Ô∏è‚É£ Implementaci√≥n de la Base de Datos Vectorial**

üìç **Archivo ****`database.py`**:

```python
import chromadb
from modules.logger import logger
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

def get_vectorstore():
    """
    Inicializa y devuelve un almac√©n de vectores utilizando ChromaDB.
    """
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        return Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    except Exception as e:
        logger.error(f"Error al conectar con la base de datos vectorial: {e}")
        return None
```

---

## **5Ô∏è‚É£ Carga y Almacenamiento de Documentos**

üìç **Archivo ****`embeddings.py`**:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from modules.database import get_vectorstore
from modules.logger import logger

def load_and_store_documents(file_content, file_name):
    """
    Carga documentos subidos a trav√©s de Streamlit, genera embeddings y los almacena en ChromaDB.
    """
    try:
        with open(f"data/documentos/{file_name}", "wb") as f:
            f.write(file_content.getbuffer())
        
        loader = TextLoader(f"data/documentos/{file_name}")
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = text_splitter.split_documents(documents)
        
        vectorstore = get_vectorstore()
        if vectorstore:
            vectorstore.add_documents(docs)
            logger.info(f"Documentos del archivo {file_name} indexados correctamente en ChromaDB.")
        else:
            logger.warning("No se pudo indexar el documento debido a un problema con la base de datos vectorial.")
    except Exception as e:
        logger.error(f"Error al cargar y almacenar documentos: {e}")
```

---

## **6Ô∏è‚É£ Implementaci√≥n del Chatbot**

üìç **Archivo ****`chatbot.py`**:

```python
from langchain_ollama import OllamaLLM
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory
from modules.database import get_vectorstore
from modules.logger import logger

def chat_with_llama(user_input, vectorstore):
    """
    Genera una respuesta utilizando el modelo LLaMA con la informaci√≥n de ChromaDB.
    """
    try:
        llm = OllamaLLM(model="llama2")
        memory = ConversationSummaryBufferMemory(llm=llm, memory_key="chat_history", return_messages=True)
        retriever = vectorstore.as_retriever() if vectorstore else None
        retrieval_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory
        )
        
        response = retrieval_chain.run(user_input) if retrieval_chain else "No se pudo generar respuesta."
        return response
    except Exception as e:
        logger.error(f"Error en la generaci√≥n de respuesta del chatbot: {e}")
        return "Lo siento, hubo un error al procesar tu solicitud."
```

---

## **7Ô∏è‚É£ Construcci√≥n de la Aplicaci√≥n con Streamlit**

üìç **Archivo ****`app.py`**:

üìç **Archivo ****`app.py`**:

```python
import streamlit as st
from modules.chatbot import chat_with_llama
from modules.embeddings import load_and_store_documents
from modules.logger import logger
from modules.database import get_vectorstore

# Configuraci√≥n de la interfaz de Streamlit
st.set_page_config(page_title="Chat con LLaMA y Base de Datos", layout="wide")
st.title("ü§ñ Chat con LLaMA y Base de Datos Vectorial")

# Cargar la base de datos vectorial una sola vez
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = get_vectorstore()
    logger.info("Base de datos vectorial cargada en memoria.")

# Secci√≥n para cargar documentos desde la interfaz
st.sidebar.header("Carga de Documentos")
uploaded_file = st.sidebar.file_uploader("Sube un archivo de texto o datos", type=["txt", "csv", "json"])

if uploaded_file is not None:
    ruta_guardado = f"data/documentos/{uploaded_file.name}"
    with open(ruta_guardado, "wb") as f:
        f.write(uploaded_file.getbuffer())
    load_and_store_documents(uploaded_file, uploaded_file.name)
    logger.info(f"Archivo {uploaded_file.name} cargado e indexado correctamente.")
    st.sidebar.success(f"Archivo {uploaded_file.name} cargado correctamente y agregado al contexto.")

# Inicializar el historial de conversaci√≥n
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar historial de conversaci√≥n
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Entrada del usuario
user_input = st.chat_input("Escribe tu mensaje...")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # Utilizar la base de datos vectorial precargada
    response = chat_with_llama(user_input, st.session_state.vectorstore)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.markdown(response)
```

---

## **8Ô∏è‚É£ Ingenier√≠a de Prompts**

## **8Ô∏è‚É£ Ingenier√≠a de Prompts para Optimizar el Chatbot**

La **ingenier√≠a de prompts** es una t√©cnica clave en la interacci√≥n con modelos de lenguaje natural. Consiste en dise√±ar instrucciones precisas y estructuradas para obtener respuestas m√°s relevantes y contextuales del chatbot.

### **üìå Fundamentos de la Ingenier√≠a de Prompts**

Los modelos de lenguaje, como LLaMA, generan respuestas bas√°ndose en patrones ling√º√≠sticos y contextos aprendidos. La calidad de la respuesta est√° directamente relacionada con la claridad, precisi√≥n y contexto del prompt ingresado.

### **üìå Tipos de Prompts y su Implementaci√≥n en el Chatbot**

#### **üîπ Preguntas Directas**

Este tipo de preguntas buscan respuestas espec√≠ficas y concretas.

‚úÖ **Ejemplo:**

```text
¬øQu√© ciudad menciona User 1 en la conversaci√≥n sobre mudanza?
```

üìç **C√≥mo funciona en el chatbot:** El modelo buscar√° en la base de datos vectorial y extraer√° la ciudad mencionada en el di√°logo.

#### **üîπ Prompts con Contexto Expandido**

Para mejorar la precisi√≥n, podemos incluir m√°s informaci√≥n en el prompt.

‚úÖ **Ejemplo:**

```text
User 1 est√° emocionado por mudarse a una nueva ciudad para seguir su sue√±o. ¬øA qu√© ciudad se refiere y por qu√©?
```

üìç **Ventaja:** Se gu√≠a al modelo a procesar informaci√≥n dentro de un marco m√°s definido.

#### **üîπ Prompts Comparativos**

Se usan para comparar informaci√≥n entre distintos fragmentos del contexto.

‚úÖ **Ejemplo:**

```text
¬øQui√©n tiene m√°s afinidad con la naturaleza, User 1 o User 2?
```

üìç **C√≥mo funciona en el chatbot:** Permite analizar distintas interacciones para evaluar qui√©n menciona m√°s actividades al aire libre.

#### **üîπ Preguntas sobre Emociones y Motivaciones**

Este tipo de preguntas buscan inferir estados emocionales y razones detr√°s de una acci√≥n en el di√°logo.

‚úÖ **Ejemplo:**

```text
¬øQu√© motiva a User 2 a ser bombero y mudarse de ciudad?
```

üìç **C√≥mo funciona en el chatbot:** Se extraer√°n indicios emocionales en el texto para justificar su decisi√≥n.

#### **üîπ Reformulaci√≥n de Preguntas Ambiguas**

Si el chatbot no responde de forma precisa, podemos hacer preguntas m√°s guiadas.

‚ùå **Pregunta Ambigua:**

```text
¬øQu√© sabe el chatbot sobre Portland?
```

‚úÖ **Mejor Pregunta:**

```text
¬øQu√© mencionan los personajes en el contexto sobre lugares emblem√°ticos en Portland?
```

üìç **C√≥mo funciona en el chatbot:** Reduce la ambig√ºedad y gu√≠a mejor la b√∫squeda en la base de datos vectorial.

### **üìå Implementaci√≥n en Streamlit**

Podemos agregar una opci√≥n en `app.py` para que los usuarios seleccionen estrategias de ingenier√≠a de prompts.

```python
st.sidebar.header("Estrategia de Ingenier√≠a de Prompts")
prompt_type = st.sidebar.selectbox("Selecciona un tipo de prompt", [
    "Pregunta Directa",
    "Contexto Expandido",
    "Comparaci√≥n",
    "Emociones y Motivaciones",
    "Reformulaci√≥n"
])
```

### **üìå Consejos para Crear Prompts M√°s Efectivos**

1Ô∏è‚É£ **S√© espec√≠fico:** Cuanto m√°s claro sea el prompt, mejor ser√° la respuesta.
2Ô∏è‚É£ **A√±ade contexto adicional:** Explicar el prop√≥sito de la pregunta mejora la precisi√≥n.
3Ô∏è‚É£ **Reformula si es necesario:** Si la respuesta es imprecisa, intenta una variaci√≥n del prompt.
4Ô∏è‚É£ **Evita preguntas demasiado generales:** Preguntas vagas pueden generar respuestas poco informativas.
5Ô∏è‚É£ **Usa ejemplos dentro del prompt:** Si es necesario, incluye referencias en la instrucci√≥n.

---

## **üéØ Resultados Esperados**

‚úÖ **Mejor precisi√≥n en las respuestas del chatbot.**\
‚úÖ **Capacidad de obtener respuestas m√°s relevantes del contexto.**\
‚úÖ **Mayor control sobre c√≥mo el chatbot interact√∫a con los datos almacenados.**

üöÄ **Con estas t√©cnicas, optimizamos la calidad de las respuestas del chatbot y aprovechamos al m√°ximo la base de datos vectorial!** üéØ

---

## **üéØ Resultados Esperados**

‚úÖ Un chatbot interactivo que puede responder preguntas sobre el contexto cargado.\
‚úÖ Integraci√≥n con **ChromaDB** para mejorar la precisi√≥n de las respuestas.\
‚úÖ Optimizaci√≥n mediante **ingenier√≠a de prompts** para mejorar la calidad de la generaci√≥n de texto.



