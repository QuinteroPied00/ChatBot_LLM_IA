# ğŸ“Œ Curso PrÃ¡ctico: ConstrucciÃ³n de un Chatbot con IA Generativa y Base de Datos Vectorial

Este curso guiarÃ¡ a los estudiantes en la creaciÃ³n de un **chatbot con IA generativa** utilizando **LLaMA, LangChain, ChromaDB y Streamlit**. Se abordarÃ¡ desde la instalaciÃ³n y configuraciÃ³n del entorno hasta la optimizaciÃ³n de respuestas a travÃ©s de tÃ©cnicas avanzadas de **ingenierÃ­a de prompts**.

---

## **1ï¸âƒ£ IntroducciÃ³n a la IA Generativa y Bases de Datos Vectoriales**

### **ğŸ“Œ Inteligencia Artificial Generativa**   

La IA generativa hace referencia a modelos capaces de **generar texto, imÃ¡genes, cÃ³digo y otros tipos de contenido** basado en patrones aprendidos a partir de grandes volÃºmenes de datos. Modelos como **LLaMA** utilizan arquitecturas de **Transformers**, que permiten procesar informaciÃ³n secuencialmente, atendiendo relaciones contextuales dentro del texto.

### **ğŸ“Œ Bases de Datos Vectoriales**

Las bases de datos vectoriales, como **ChromaDB**, permiten almacenar informaciÃ³n en forma de **vectores embebidos**. Esto facilita la bÃºsqueda y recuperaciÃ³n eficiente de informaciÃ³n basada en **similitud semÃ¡ntica**, en lugar de coincidencias exactas de palabras clave.

ğŸ“ **CÃ³mo lo implementamos en este curso:**

- mejoraUtilizamos **HuggingFaceEmbeddings** para convertir el texto en representaciones vectoriales.
- Almacenamos estos vectores en **ChromaDB** para facilitar consultas en lenguaje natural.
- Integramos estas consultas en **LangChain**, combinando bases de datos y modelos generativos para generar respuestas relevantes.

---

## **2ï¸âƒ£ InstalaciÃ³n del Entorno de Desarrollo**

### **ğŸ”¹ Paso 1: Crear y Activar un Entorno Virtual**

```bash
# Crear un entorno virtual
python3 -m venv venv

# Activar el entorno virtual (Linux/macOS)
source venv/bin/activate

# Activar el entorno virtual (Windows)
venv\Scripts\activate
```

### **ğŸ”¹ Paso 2: Instalar las Dependencias**

```bash
pip install streamlit langchain langchain-community ollama chromadb sentence-transformers
```

---

## **3ï¸âƒ£ ConfiguraciÃ³n del Logger**

Para facilitar la depuraciÃ³n y monitoreo del chatbot, configuramos un sistema de **logging** que capture eventos clave en la ejecuciÃ³n.

ğŸ“ **Archivo ****`logger.py`**:

```python
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
```

---

## **4ï¸âƒ£ ImplementaciÃ³n de la Base de Datos Vectorial**

ğŸ“ **Archivo ****`database.py`**:

```python
import chromadb
from modules.logger import logger
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

def get_vectorstore():
    """
    Inicializa y devuelve un almacÃ©n de vectores utilizando ChromaDB.
    """
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        return Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    except Exception as e:
        logger.error(f"Error al conectar con la base de datos vectorial: {e}")
        return None
```

---

## **5ï¸âƒ£ Carga y Almacenamiento de Documentos**

ğŸ“ **Archivo ****`embeddings.py`**:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from modules.database import get_vectorstore
from modules.logger import logger

def load_and_store_documents(file_content, file_name):
    """
    Carga documentos subidos a travÃ©s de Streamlit, genera embeddings y los almacena en ChromaDB.
    """
    try:
        with open(f"data/documentos/{file_name}", "wb") as f:
            f.write(file_content.getbuffer())
        
        loader = TextLoader(f"data/documentos/{file_name}")
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = text_splitter.split_documents(documents)
        
        vectorstore = get_vectorstore()
        if vectorstore:
            vectorstore.add_documents(docs)
            logger.info(f"Documentos del archivo {file_name} indexados correctamente en ChromaDB.")
        else:
            logger.warning("No se pudo indexar el documento debido a un problema con la base de datos vectorial.")
    except Exception as e:
        logger.error(f"Error al cargar y almacenar documentos: {e}")
```

---

## **6ï¸âƒ£ ImplementaciÃ³n del Chatbot**

ğŸ“ **Archivo ****`chatbot.py`**:

```python
from langchain_ollama import OllamaLLM
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory
from modules.database import get_vectorstore
from modules.logger import logger

def chat_with_llama(user_input, vectorstore):
    """
    Genera una respuesta utilizando el modelo LLaMA con la informaciÃ³n de ChromaDB.
    """
    try:
        llm = OllamaLLM(model="llama2")
        memory = ConversationSummaryBufferMemory(llm=llm, memory_key="chat_history", return_messages=True)
        retriever = vectorstore.as_retriever() if vectorstore else None
        retrieval_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory
        )
        
        response = retrieval_chain.run(user_input) if retrieval_chain else "No se pudo generar respuesta."
        return response
    except Exception as e:
        logger.error(f"Error en la generaciÃ³n de respuesta del chatbot: {e}")
        return "Lo siento, hubo un error al procesar tu solicitud."
```

---

## **7ï¸âƒ£ ConstrucciÃ³n de la AplicaciÃ³n con Streamlit**

ğŸ“ **Archivo ****`app.py`**:

ğŸ“ **Archivo ****`app.py`**:

```python
import streamlit as st
from modules.chatbot import chat_with_llama
from modules.embeddings import load_and_store_documents
from modules.logger import logger
from modules.database import get_vectorstore

# ConfiguraciÃ³n de la interfaz de Streamlit
st.set_page_config(page_title="Chat con LLaMA y Base de Datos", layout="wide")
st.title("ğŸ¤– Chat con LLaMA y Base de Datos Vectorial")

# Cargar la base de datos vectorial una sola vez
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = get_vectorstore()
    logger.info("Base de datos vectorial cargada en memoria.")

# SecciÃ³n para cargar documentos desde la interfaz
st.sidebar.header("Carga de Documentos")
uploaded_file = st.sidebar.file_uploader("Sube un archivo de texto o datos", type=["txt", "csv", "json"])

if uploaded_file is not None:
    ruta_guardado = f"data/documentos/{uploaded_file.name}"
    with open(ruta_guardado, "wb") as f:
        f.write(uploaded_file.getbuffer())
    load_and_store_documents(uploaded_file, uploaded_file.name)
    logger.info(f"Archivo {uploaded_file.name} cargado e indexado correctamente.")
    st.sidebar.success(f"Archivo {uploaded_file.name} cargado correctamente y agregado al contexto.")

# Inicializar el historial de conversaciÃ³n
if "messages" not in st.session_state:
    st.session_state.messages = []

# Mostrar historial de conversaciÃ³n
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Entrada del usuario
user_input = st.chat_input("Escribe tu mensaje...")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # Utilizar la base de datos vectorial precargada
    response = chat_with_llama(user_input, st.session_state.vectorstore)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.markdown(response)
```

---

## **8ï¸âƒ£ IngenierÃ­a de Prompts**

## **8ï¸âƒ£ IngenierÃ­a de Prompts para Optimizar el Chatbot**

La **ingenierÃ­a de prompts** es una tÃ©cnica clave en la interacciÃ³n con modelos de lenguaje natural. Consiste en diseÃ±ar instrucciones precisas y estructuradas para obtener respuestas mÃ¡s relevantes y contextuales del chatbot.

### **ğŸ“Œ Fundamentos de la IngenierÃ­a de Prompts**

Los modelos de lenguaje, como LLaMA, generan respuestas basÃ¡ndose en patrones lingÃ¼Ã­sticos y contextos aprendidos. La calidad de la respuesta estÃ¡ directamente relacionada con la claridad, precisiÃ³n y contexto del prompt ingresado.

### **ğŸ“Œ Tipos de Prompts y su ImplementaciÃ³n en el Chatbot**

#### **ğŸ”¹ Preguntas Directas**

Este tipo de preguntas buscan respuestas especÃ­ficas y concretas.

âœ… **Ejemplo:**

```text
Â¿QuÃ© ciudad menciona User 1 en la conversaciÃ³n sobre mudanza?
```

ğŸ“ **CÃ³mo funciona en el chatbot:** El modelo buscarÃ¡ en la base de datos vectorial y extraerÃ¡ la ciudad mencionada en el diÃ¡logo.

#### **ğŸ”¹ Prompts con Contexto Expandido**

Para mejorar la precisiÃ³n, podemos incluir mÃ¡s informaciÃ³n en el prompt.

âœ… **Ejemplo:**

```text
User 1 estÃ¡ emocionado por mudarse a una nueva ciudad para seguir su sueÃ±o. Â¿A quÃ© ciudad se refiere y por quÃ©?
```

ğŸ“ **Ventaja:** Se guÃ­a al modelo a procesar informaciÃ³n dentro de un marco mÃ¡s definido.

#### **ğŸ”¹ Prompts Comparativos**

Se usan para comparar informaciÃ³n entre distintos fragmentos del contexto.

âœ… **Ejemplo:**

```text
Â¿QuiÃ©n tiene mÃ¡s afinidad con la naturaleza, User 1 o User 2?
```

ğŸ“ **CÃ³mo funciona en el chatbot:** Permite analizar distintas interacciones para evaluar quiÃ©n menciona mÃ¡s actividades al aire libre.

#### **ğŸ”¹ Preguntas sobre Emociones y Motivaciones**

Este tipo de preguntas buscan inferir estados emocionales y razones detrÃ¡s de una acciÃ³n en el diÃ¡logo.

âœ… **Ejemplo:**

```text
Â¿QuÃ© motiva a User 2 a ser bombero y mudarse de ciudad?
```

ğŸ“ **CÃ³mo funciona en el chatbot:** Se extraerÃ¡n indicios emocionales en el texto para justificar su decisiÃ³n.

#### **ğŸ”¹ ReformulaciÃ³n de Preguntas Ambiguas**

Si el chatbot no responde de forma precisa, podemos hacer preguntas mÃ¡s guiadas.

âŒ **Pregunta Ambigua:**

```text
Â¿QuÃ© sabe el chatbot sobre Portland?
```

âœ… **Mejor Pregunta:**

```text
Â¿QuÃ© mencionan los personajes en el contexto sobre lugares emblemÃ¡ticos en Portland?
```

ğŸ“ **CÃ³mo funciona en el chatbot:** Reduce la ambigÃ¼edad y guÃ­a mejor la bÃºsqueda en la base de datos vectorial.

### **ğŸ“Œ ImplementaciÃ³n en Streamlit**

Podemos agregar una opciÃ³n en `app.py` para que los usuarios seleccionen estrategias de ingenierÃ­a de prompts.

```python
st.sidebar.header("Estrategia de IngenierÃ­a de Prompts")
prompt_type = st.sidebar.selectbox("Selecciona un tipo de prompt", [
    "Pregunta Directa",
    "Contexto Expandido",
    "ComparaciÃ³n",
    "Emociones y Motivaciones",
    "ReformulaciÃ³n"
])
```

### **ğŸ“Œ Consejos para Crear Prompts MÃ¡s Efectivos**

1ï¸âƒ£ **SÃ© especÃ­fico:** Cuanto mÃ¡s claro sea el prompt, mejor serÃ¡ la respuesta.
2ï¸âƒ£ **AÃ±ade contexto adicional:** Explicar el propÃ³sito de la pregunta mejora la precisiÃ³n.
3ï¸âƒ£ **Reformula si es necesario:** Si la respuesta es imprecisa, intenta una variaciÃ³n del prompt.
4ï¸âƒ£ **Evita preguntas demasiado generales:** Preguntas vagas pueden generar respuestas poco informativas.
5ï¸âƒ£ **Usa ejemplos dentro del prompt:** Si es necesario, incluye referencias en la instrucciÃ³n.

---

## **ğŸ¯ Resultados Esperados**

âœ… **Mejor precisiÃ³n en las respuestas del chatbot.**\
âœ… **Capacidad de obtener respuestas mÃ¡s relevantes del contexto.**\
âœ… **Mayor control sobre cÃ³mo el chatbot interactÃºa con los datos almacenados.**

ğŸš€ **Con estas tÃ©cnicas, optimizamos la calidad de las respuestas del chatbot y aprovechamos al mÃ¡ximo la base de datos vectorial!** ğŸ¯

---

## **ğŸ¯ Resultados Esperados**

âœ… Un chatbot interactivo que puede responder preguntas sobre el contexto cargado.\
âœ… IntegraciÃ³n con **ChromaDB** para mejorar la precisiÃ³n de las respuestas.\
âœ… OptimizaciÃ³n mediante **ingenierÃ­a de prompts** para mejorar la calidad de la generaciÃ³n de texto.



