# üìå Curso Pr√°ctico: Construcci√≥n de un Chatbot con IA Generativa y Base de Datos Vectorial

Este curso guiar√° a los estudiantes paso a paso en la creaci√≥n de un **chatbot con IA generativa** usando **LLaMA, LangChain, ChromaDB y Streamlit**.

---

## **Conceptos Clave Implementados en el Chatbot**

### **üìå 1. Embeddings** 
Los **embeddings** son representaciones vectoriales de palabras o frases que permiten que el chatbot entienda relaciones sem√°nticas entre t√©rminos. En nuestro chatbot, los embeddings se generan con **HuggingFaceEmbeddings** y se almacenan en ChromaDB para ser utilizados en la b√∫squeda de informaci√≥n relevante.

üìç **D√≥nde se usa en el c√≥digo:**
- En `database.py`, la funci√≥n `get_vectorstore()` genera embeddings usando el modelo `sentence-transformers/all-MiniLM-L6-v2`.
- En `embeddings.py`, los documentos cargados se dividen en fragmentos y se convierten en embeddings antes de almacenarlos en la base de datos.

```python
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore.add_documents(docs)
```

### **üìå 2. Modelos Transformers**
Los **transformers** son modelos de IA especializados en el procesamiento del lenguaje natural (NLP). Se basan en la arquitectura de atenci√≥n para comprender el contexto de las palabras en una oraci√≥n. En este proyecto, utilizamos **LLaMA**, un modelo basado en transformers, para generar respuestas contextuales.

üìç **D√≥nde se usa en el c√≥digo:**
- En `chatbot.py`, el modelo **OllamaLLM** es el encargado de generar respuestas basadas en la consulta del usuario.

```python
llm = OllamaLLM(model="llama2")
```

### **üìå 3. Cadenas (Chains) en LangChain**
Las **cadenas (Chains)** en LangChain permiten conectar diferentes componentes, como modelos de lenguaje y bases de datos vectoriales, para construir flujos de trabajo complejos. En nuestro chatbot, utilizamos una cadena de recuperaci√≥n conversacional (`ConversationalRetrievalChain`) para responder a preguntas bas√°ndose en el contexto almacenado.

üìç **D√≥nde se usa en el c√≥digo:**
- En `chatbot.py`, la cadena `ConversationalRetrievalChain` se configura para recuperar informaci√≥n relevante antes de generar la respuesta.

```python
retrieval_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory
)
```

### **üìå 4. Base de Datos Vectorial (ChromaDB)**
Las bases de datos vectoriales almacenan informaci√≥n en forma de embeddings para permitir una b√∫squeda r√°pida y eficiente. En este chatbot, utilizamos **ChromaDB** para indexar y recuperar documentos relevantes.

üìç **D√≥nde se usa en el c√≥digo:**
- En `database.py`, `get_vectorstore()` inicializa y gestiona la base de datos vectorial.
- En `embeddings.py`, los documentos se convierten en embeddings y se almacenan en ChromaDB.
- En `chatbot.py`, `get_vectorstore().as_retriever()` permite la b√∫squeda en la base de datos.

```python
vectorstore = get_vectorstore()
retriever = vectorstore.as_retriever()
```

---

## **1Ô∏è‚É£ Instalaci√≥n de Dependencias**

### **üîπ Paso 1: Crear y Activar un Entorno Virtual**

Ejecuta los siguientes comandos en la terminal:

```bash
# Crear un entorno virtual
python3 -m venv venv

# Activar el entorno virtual (Linux/macOS)
source venv/bin/activate

# Activar el entorno virtual (Windows)
venv\Scripts\activate
```

### **üîπ Paso 2: Instalar las Dependencias**

Ejecuta:

```bash
pip install streamlit langchain langchain-community ollama chromadb sentence-transformers
```

---

## **2Ô∏è‚É£ Configuraci√≥n del Logger**

Creamos un archivo `logger.py` dentro de la carpeta `modules/` con el siguiente c√≥digo:

```python
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
```

---

## **3Ô∏è‚É£ Configuraci√≥n de la Base de Datos Vectorial**

Creamos un archivo `database.py` dentro de la carpeta `modules/` con el siguiente c√≥digo:

```python
import chromadb
from modules.logger import logger
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

def get_vectorstore():
    """
    Inicializa y devuelve un almac√©n de vectores utilizando ChromaDB.
    Utiliza HuggingFaceEmbeddings para generar los embeddings de los documentos.
    """
    try:
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        return Chroma(persist_directory="./chroma_db", embedding_function=embeddings)
    except Exception as e:
        logger.error(f"Error al conectar con la base de datos vectorial: {e}")
        return None
```

---

## **4Ô∏è‚É£ Cargar Documentos en la Base de Datos**

Creamos un archivo `embeddings.py` dentro de la carpeta `modules/` con el siguiente c√≥digo:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from modules.database import get_vectorstore
from modules.logger import logger

def load_and_store_documents(file_content, file_name):
    """
    Carga documentos subidos a trav√©s de Streamlit, genera embeddings y los almacena en ChromaDB.
    """
    try:
        with open(f"data/documentos/{file_name}", "wb") as f:
            f.write(file_content.getbuffer())
        
        loader = TextLoader(f"data/documentos/{file_name}")
        documents = loader.load()
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = text_splitter.split_documents(documents)
        
        vectorstore = get_vectorstore()
        if vectorstore:
            vectorstore.add_documents(docs)
            logger.info(f"Documentos del archivo {file_name} indexados correctamente en ChromaDB.")
        else:
            logger.warning("No se pudo indexar el documento debido a un problema con la base de datos vectorial.")
    except Exception as e:
        logger.error(f"Error al cargar y almacenar documentos: {e}")
```

---

## **5Ô∏è‚É£ Implementar el Chatbot y Almacenar el Historial**

Creamos un archivo `chatbot.py` dentro de la carpeta `modules/` con el siguiente c√≥digo:

```python
from langchain_ollama import OllamaLLM
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationSummaryBufferMemory
from modules.database import get_vectorstore
from modules.logger import logger
import json
import os

HISTORY_FILE = "data/historial/search_history.json"

def ensure_history_file():
    """
    Verifica la existencia del archivo de historial y lo crea si no existe.
    """
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    if not os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "w") as f:
            json.dump([], f)

def save_search(query, response):
    """
    Guarda una consulta y su respuesta en el historial de b√∫squedas.
    """
    ensure_history_file()
    with open(HISTORY_FILE, "r+") as f:
        history = json.load(f)
        history.append({"query": query, "response": response})
        f.seek(0)
        json.dump(history, f, indent=4)

def chat_with_llama(user_input):
    """
    Genera una respuesta a partir de una consulta utilizando el modelo LLaMA.
    """
    try:
        llm = OllamaLLM(model="llama2")
        memory = ConversationSummaryBufferMemory(llm=llm, memory_key="chat_history", return_messages=True)
        vectorstore = get_vectorstore()
        retriever = vectorstore.as_retriever() if vectorstore else None
        retrieval_chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory
        )
        
        response = retrieval_chain.run(user_input) if retrieval_chain else "Error: No se pudo generar una respuesta debido a problemas con el contexto."
        save_search(user_input, response)
        return response
    except Exception as e:
        logger.error(f"Error en la generaci√≥n de respuesta del chatbot: {e}")
        return "Lo siento, hubo un error al procesar tu solicitud."
```

---

## **6Ô∏è‚É£ Construcci√≥n de la Aplicaci√≥n con Streamlit**

Creamos `app.py` en la ra√≠z del proyecto con la siguiente configuraci√≥n:

```python
import streamlit as st
from modules.chatbot import chat_with_llama
from modules.embeddings import load_and_store_documents
from modules.logger import logger

st.set_page_config(page_title="Chat con LLaMA y Base de Datos", layout="wide")
st.title("ü§ñ Chat con LLaMA y Base de Datos Vectorial")

st.sidebar.header("Carga de Documentos")
uploaded_file = st.sidebar.file_uploader("Sube un archivo de texto o datos", type=["txt", "csv", "json"])
if uploaded_file is not None:
    ruta_guardado = f"data/documentos/{uploaded_file.name}"
    with open(ruta_guardado, "wb") as f:
        f.write(uploaded_file.getbuffer())
    load_and_store_documents(uploaded_file, uploaded_file.name)
    logger.info(f"Archivo {uploaded_file.name} cargado e indexado correctamente.")
    st.sidebar.success(f"Archivo {uploaded_file.name} cargado correctamente y agregado al contexto.")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("Escribe tu mensaje...")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    
    response = chat_with_llama(user_input)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.markdown(response)
```

---

## **7Ô∏è‚É£ Ejecutar el Proyecto**

### **üîπ Paso 1: Iniciar Ollama**

```bash
ollama serve
```

### **üîπ Paso 2: Ejecutar Streamlit**

```bash
streamlit run app.py
```

---

## **üéØ Resultados Esperados**

‚úÖ Un chatbot interactivo que puede responder preguntas.
‚úÖ Capacidad de **cargar documentos** y usarlos como contexto.
‚úÖ Integraci√≥n con **ChromaDB** para mejorar la precisi√≥n de las respuestas.
‚úÖ **Historial de b√∫squeda almacenado** para consultar interacciones previas.


